---
phase: "04"
plan: "04"
title: "Bulk Export Engine, JQL Search & Full Wiring"
wave: 2
depends_on:
  - "01"
  - "02"
  - "03"
skills_used:
  - python-testing-patterns
must_haves:
  - "`src/jarkdown/bulk_exporter.py` exists with `BulkExporter` class using `asyncio.Semaphore(concurrency)` and `asyncio.gather(*tasks, return_exceptions=True)`"
  - "`AsyncJiraApiClient.search_jql(jql, max_results)` paginates via `nextPageToken` until exhausted or max_results reached"
  - "`BulkExporter.generate_index_md()` produces a Markdown table with columns: Key (linked to local .md), Summary, Status, Type, Assignee, Result; header shows export count and date"
  - "Progress counter prints to stderr: `Exporting 3/10...` (\\r overwrite pattern)"
  - "Exit code 0 if all succeed; exit code 1 if any failures; failed issues appear in index.md Result column with error reason"
  - "`jarkdown bulk PROJ-1 PROJ-2` runs real export (not stub) after this plan"
  - "`jarkdown query 'project = FOO'` runs real JQL search and export after this plan"
  - "`pytest tests/test_bulk_exporter.py` passes including partial failure, pagination, and index.md scenarios"
---

## Objective

Implement the complete bulk export engine: `BulkExporter` class with semaphore-limited concurrency, continue-and-report error handling, and index.md generation. Add `search_jql()` to the async JiraApiClient for JQL-based discovery. Wire `bulk` and `query` subcommands in `jarkdown.py` to replace the stubs from Plan 02. This is the final integration plan for Phase 4.

## Context

Read these files before starting (post wave-1 versions):
- `@src/jarkdown/jira_api_client.py` — async JiraApiClient (from Plan 01); add search_jql() method here
- `@src/jarkdown/jarkdown.py` — subcommand CLI (from Plan 02); replace _handle_bulk and _handle_query stubs
- `@src/jarkdown/retry.py` — retry infrastructure (from Plan 03); use retry_with_backoff in search_jql
- `@src/jarkdown/attachment_handler.py` — async AttachmentHandler (from Plan 01)
- `@src/jarkdown/exceptions.py` — exception hierarchy; may need new BulkExportError
- `@.vbw-planning/phases/04-bulk-export-jql/04-CONTEXT.md` — ALL design decisions (output structure, index.md, partial failure semantics, batch-name flag, --batch-name directory layout)
- `@.vbw-planning/phases/04-bulk-export-jql/04-RESEARCH.md` — JQL pagination (nextPageToken), semaphore patterns, asyncio.gather return_exceptions, index.md format

Key design decisions from context:
- Default concurrency: 3 (configurable via --concurrency flag already added in Plan 02)
- Semaphore shared between issue fetching and attachment downloads
- Output: flat siblings by default — each issue KEY/ directory under --output root
- Optional --batch-name → wrap in `{batch-name}/` subdirectory
- index.md at output root (or batch directory root)
- Continue-and-report: 404/401 per-issue → record in failures, move to next issue
- Transient errors (429, 503) → retry via retry_with_backoff
- Exit code: 0 all succeed, 1 any failures
- Progress to stderr: `\rExporting {n}/{total}...` with flush

JQL Pagination (critical from research):
- Jira v3 search endpoint: `GET /rest/api/3/search/jql`
- Params: `jql`, `maxResults` (page size, default 50), `nextPageToken` (for subsequent pages)
- Response: `{"issues": [...], "nextPageToken": "..."}` — nextPageToken absent or null when done
- Must paginate sequentially (cannot parallelize across pages)

Merge safety: This is the only wave-2 plan. Files modified: `jira_api_client.py`, `jarkdown.py` (both updated in wave 1), new `bulk_exporter.py`, new `tests/test_bulk_exporter.py`. No conflicts.

## Tasks

### Task 1: Add search_jql() to JiraApiClient

**Files:** `src/jarkdown/jira_api_client.py`

Add `search_jql()` async method to the existing `JiraApiClient` class:

```python
async def search_jql(self, jql: str, max_results: int = 50) -> list:
    """Search for issues matching a JQL query, paginating via nextPageToken.

    Args:
        jql: JQL query string (e.g., 'project = FOO AND status = Done')
        max_results: Maximum total issues to return across all pages.

    Returns:
        list: Issue data dicts from Jira API (keys, fields — light-weight summary data)

    Raises:
        JiraApiError: If any API call fails
        AuthenticationError: On 401
    """
    from .retry import retry_with_backoff, DEFAULT_RETRY

    url = f"{self.api_base}/search/jql"
    issues = []
    next_page_token = None
    page_size = min(max_results, 50)  # Jira max per page is 50

    while len(issues) < max_results:
        remaining = max_results - len(issues)
        params = {
            "jql": jql,
            "maxResults": min(remaining, page_size),
            "fields": "summary,issuetype,status,assignee",
        }
        if next_page_token:
            params["nextPageToken"] = next_page_token

        async def _fetch_page():
            async with self.session.get(url, params=params) as resp:
                resp.raise_for_status()
                return await resp.json()

        try:
            data = await retry_with_backoff(_fetch_page, config=DEFAULT_RETRY)
        except aiohttp.ClientResponseError as e:
            if e.status == 401:
                raise AuthenticationError(
                    "Authentication failed during JQL search.",
                    status_code=401,
                )
            raise JiraApiError(f"JQL search failed: HTTP {e.status}", status_code=e.status)

        page_issues = data.get("issues", [])
        issues.extend(page_issues)

        next_page_token = data.get("nextPageToken")
        if not next_page_token or not page_issues:
            break

    return issues[:max_results]
```

**Tests:** Covered in Task 5.

**Commit:** `feat(phase-04): add search_jql with nextPageToken pagination to JiraApiClient`

---

### Task 2: Create BulkExporter class

**Files:** `src/jarkdown/bulk_exporter.py`

Create `src/jarkdown/bulk_exporter.py`:

```python
"""Bulk export engine for exporting multiple Jira issues concurrently."""

import asyncio
import sys
import logging
from datetime import datetime, timezone
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Optional, Tuple

from .exceptions import JarkdownError

logger = logging.getLogger(__name__)


@dataclass
class ExportResult:
    """Result of a single issue export attempt."""
    issue_key: str
    success: bool
    output_path: Optional[Path] = None
    error: Optional[str] = None


class BulkExporter:
    """Orchestrates concurrent export of multiple Jira issues.

    Args:
        api_client: Async JiraApiClient context (must be entered)
        concurrency: Maximum simultaneous exports (default: 3)
        output_dir: Root output directory (default: cwd)
        batch_name: Optional subdirectory wrapper for batch output
        refresh_fields: Force refresh field metadata cache
        include_fields: CSV of custom fields to include
        exclude_fields: CSV of custom fields to exclude
    """

    def __init__(self, api_client, concurrency=3, output_dir=None,
                 batch_name=None, refresh_fields=False,
                 include_fields=None, exclude_fields=None):
        self.api_client = api_client
        self.semaphore = asyncio.Semaphore(concurrency)
        self.output_dir = Path(output_dir) if output_dir else Path.cwd()
        if batch_name:
            self.output_dir = self.output_dir / batch_name
        self.refresh_fields = refresh_fields
        self.include_fields = include_fields
        self.exclude_fields = exclude_fields
```

Implement `async def export_bulk(self, issue_keys: List[str]) -> Tuple[List[ExportResult], List[ExportResult]]`:
- Create tasks: `[self._export_one(key, i+1, total) for i, key in enumerate(issue_keys)]`
- Use `asyncio.gather(*tasks, return_exceptions=True)`
- Process results: separate successes and failures
- Return `(successes, failures)`

Implement `async def _export_one(self, issue_key, n, total) -> ExportResult`:
- Acquire `self.semaphore` via `async with self.semaphore:`
- Print progress: `print(f"\rExporting {n}/{total}... ({issue_key})", end="", flush=True, file=sys.stderr)`
- Do NOT import from `jarkdown.jarkdown` — that creates a circular import. Instead, replicate the export logic directly: `await api_client.fetch_issue(issue_key)` → `await attachment_handler.download_all_attachments(...)` → `markdown_converter.compose_markdown(...)` → write files. This is the same sequence as `export_issue()` but inlined into `_export_one`.
- On success: return `ExportResult(issue_key, success=True, output_path=path)`
- On `IssueNotFoundError`, `AuthenticationError` (non-retryable): return `ExportResult(issue_key, success=False, error=str(e))`
- On `JarkdownError`: return `ExportResult(issue_key, success=False, error=str(e))`

After `asyncio.gather`, print newline to stderr to end progress line.

**Tests:** Covered in Task 5.

**Commit:** `feat(phase-04): add BulkExporter class with semaphore concurrency`

---

### Task 3: Add generate_index_md() to BulkExporter

**Files:** `src/jarkdown/bulk_exporter.py`

Add `generate_index_md(self, results: List[ExportResult], all_issues_data: dict) -> str` method:

The index.md content format:
```markdown
# Export Summary

Exported: 8 of 10 issues | Date: 2026-02-17 | Failed: 2

| Key | Summary | Status | Type | Assignee | Result |
|-----|---------|--------|------|----------|--------|
| [PROJ-1](PROJ-1/PROJ-1.md) | Fix login bug | Done | Bug | jane.doe | ✓ |
| [PROJ-2](PROJ-2/PROJ-2.md) | Add dark mode | In Progress | Story | john.doe | ✓ |
| [PROJ-3](#) | Deploy script | - | - | - | ✗ Issue not found |
```

Implementation:
- Accept `results` list of ExportResult and `all_issues_data` dict (key → issue data from API, for summary/status/type/assignee columns)
- Sort by issue key for deterministic output
- For successes: link `[KEY](KEY/KEY.md)` (relative path from index.md location)
- For failures: link `[KEY](#)` (no local file), Result column shows ✗ + error reason
- Header: count succeeded/total, ISO date (date only, no time), failed count
- Write index.md to `self.output_dir / "index.md"`

Add `async def write_index_md(self, results, issues_data)` as the public entry point that calls `generate_index_md()` and writes the file using `asyncio.to_thread(path.write_text, content, encoding="utf-8")`.

**Tests:** Covered in Task 5.

**Commit:** `feat(phase-04): add generate_index_md to BulkExporter`

---

### Task 4: Wire bulk and query subcommands in jarkdown.py

**Files:** `src/jarkdown/jarkdown.py`

Replace the stub `_handle_bulk()` and `_handle_query()` functions with real implementations:

```python
def _handle_bulk(args):
    """Handle 'bulk' subcommand: export multiple issues by key."""
    domain, email, api_token = _load_credentials()  # extracted helper
    asyncio.run(_async_bulk(args, domain, email, api_token))


async def _async_bulk(args, domain, email, api_token):
    async with JiraApiClient(domain, email, api_token) as client:
        exporter = BulkExporter(
            client,
            concurrency=args.concurrency,
            output_dir=args.output,
            batch_name=getattr(args, "batch_name", None),
            refresh_fields=args.refresh_fields,
            include_fields=args.include_fields,
            exclude_fields=args.exclude_fields,
        )
        successes, failures = await exporter.export_bulk(args.issue_keys)
        await exporter.write_index_md(successes + failures, {})
        _print_summary(successes, failures)
        if failures:
            sys.exit(1)


def _handle_query(args):
    """Handle 'query' subcommand: export issues from JQL."""
    domain, email, api_token = _load_credentials()
    asyncio.run(_async_query(args, domain, email, api_token))


async def _async_query(args, domain, email, api_token):
    async with JiraApiClient(domain, email, api_token) as client:
        print(f"Searching: {args.jql}", file=sys.stderr)
        issues = await client.search_jql(args.jql, max_results=args.max_results)
        if not issues:
            print("No issues found.", file=sys.stderr)
            return
        issue_keys = [i["key"] for i in issues]
        print(f"Found {len(issue_keys)} issues.", file=sys.stderr)
        exporter = BulkExporter(
            client,
            concurrency=args.concurrency,
            output_dir=args.output,
            batch_name=getattr(args, "batch_name", None),
        )
        successes, failures = await exporter.export_bulk(issue_keys)
        # Build issues_data dict from search results for index
        issues_data = {i["key"]: i for i in issues}
        await exporter.write_index_md(successes + failures, issues_data)
        _print_summary(successes, failures)
        if failures:
            sys.exit(1)
```

Add `_load_credentials()` helper that extracts the env var loading/validation from `_handle_export`, and `_print_summary(successes, failures)` that prints completion stats to stderr.

Import `BulkExporter` from `.bulk_exporter` at the top of jarkdown.py.

**Tests:** Covered in Task 5.

**Commit:** `feat(phase-04): wire bulk and query subcommands to BulkExporter`

---

### Task 5: Write comprehensive tests/test_bulk_exporter.py

**Files:** `tests/test_bulk_exporter.py`

Create `tests/test_bulk_exporter.py`:

```python
import asyncio
import pytest
from pathlib import Path
from unittest.mock import AsyncMock, MagicMock, patch
from jarkdown.bulk_exporter import BulkExporter, ExportResult
from jarkdown.exceptions import IssueNotFoundError, JiraApiError
```

**TestBulkExporterInit** class:
- `test_default_concurrency`: BulkExporter(client) → semaphore._value == 3
- `test_custom_concurrency`: BulkExporter(client, concurrency=5) → semaphore._value == 5
- `test_batch_name_creates_subdir`: output_dir + batch_name → self.output_dir has batch_name appended

**TestExportBulk** class (use tmp_path fixture):
- `test_all_succeed`: Mock export_issue succeeds for all 3 keys → 3 successes, 0 failures
- `test_partial_failure_continues`: Mock raises IssueNotFoundError for key 2 of 3 → 2 successes, 1 failure; other keys exported
- `test_all_fail`: Mock raises JiraApiError for all → 3 failures, 0 successes
- `test_semaphore_limits_concurrency`: With concurrency=1 and 5 issues, verify sequential execution via call ordering

**TestSearchJql** class (mocking aiohttp via aioresponses):
- `test_single_page_no_next_token`: Returns 3 issues, no nextPageToken → returns all 3
- `test_pagination_two_pages`: First page returns nextPageToken, second page returns issues + null token → all issues returned
- `test_max_results_respected`: max_results=2 but API returns 5 → only 2 returned
- `test_empty_result`: API returns empty issues list → returns []

**TestGenerateIndexMd** class (use tmp_path):
- `test_header_contains_count_and_date`: generate_index_md returns string with "Exported: 2 of 3 issues"
- `test_success_row_has_link`: Success ExportResult → row contains `[KEY](KEY/KEY.md)`
- `test_failure_row_has_error`: Failure ExportResult → row contains ✗ and error reason
- `test_writes_file_to_output_dir`: write_index_md creates index.md at output_dir/index.md
- `test_sorted_by_key`: Results in arbitrary order → index.md rows sorted by issue key

Run: `pytest tests/test_bulk_exporter.py -v` — all tests must pass.

**Commit:** `test(phase-04): add comprehensive bulk exporter tests`

## Verification

```bash
pytest tests/test_bulk_exporter.py -v
pytest tests/ -v  # all 187+ tests pass (original 187 + new phase-04 tests)
# Integration smoke with mock env:
python -c "
from jarkdown.bulk_exporter import BulkExporter, ExportResult
import asyncio
r = ExportResult('PROJ-1', True)
print('BulkExporter importable, ExportResult:', r)
"
# CLI help (no credentials needed):
python -m jarkdown.jarkdown bulk --help
python -m jarkdown.jarkdown query --help
```

## Success Criteria

- `BulkExporter` uses `asyncio.Semaphore(concurrency)` and `asyncio.gather(return_exceptions=True)`
- `search_jql()` paginates correctly via `nextPageToken`; stops at `max_results`
- `generate_index_md()` produces Markdown table with correct columns and linked paths
- Progress counter on stderr: `\rExporting N/total...` per-issue
- Exit code 1 when any failures; exit code 0 when all succeed
- `bulk` subcommand no longer prints "not yet implemented"
- `query` subcommand performs real JQL search and bulk export
- Failed issues appear in index.md Result column with error reason
- All new + existing tests pass
