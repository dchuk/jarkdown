---
phase: "02"
plan: "02"
title: "Dependencies, API & CLI Args"
wave: 1
depends_on: []
skills_used:
  - python-testing-patterns
must_haves:
  - "New file `src/jarkdown/field_cache.py` with `FieldMetadataCache` class"
  - "`FieldMetadataCache` stores field metadata JSON under XDG-compliant cache dir (`~/.config/jarkdown/`)"
  - "Cache file per domain: `fields-{domain}.json` with embedded timestamp"
  - "24-hour TTL — `is_stale()` returns True when cache older than 24 hours"
  - "`JiraApiClient.fetch_fields()` calls `GET /rest/api/3/field` and returns raw JSON list"
  - "Graceful degradation: if cache is stale and API unreachable, return stale cache with warning logged"
  - "`platformdirs>=4.0.0` and `tomli>=2.0.0;python_version<'3.11'` added to pyproject.toml"
  - "`--refresh-fields`, `--include-fields`, `--exclude-fields` CLI arguments added to parser"
  - "`pytest tests/test_field_cache.py` passes"
---

## Objective

Add all new dependencies to pyproject.toml, create the field metadata caching system, add the `fetch_fields()` API method, and add all new CLI arguments. This plan owns pyproject.toml and jarkdown.py modifications for Wave 1.

## Context

Read these files before starting:
- `@src/jarkdown/jira_api_client.py` — add `fetch_fields()` method to this class
- `@src/jarkdown/jarkdown.py` — add CLI arguments to the parser
- `@pyproject.toml` — add new dependencies
- `@.vbw-planning/phases/02-custom-fields-adf/02-CONTEXT.md` — cache design decisions
- `@.vbw-planning/phases/02-custom-fields-adf/02-RESEARCH.md` — API details, cache strategy, TOML lib notes

Merge safety: This plan runs in parallel with Plans 01 and 03 (same wave). Plan 01 modifies `markdown_converter.py` only. Plan 03 creates new file `config_manager.py` and test files only. This plan modifies `jira_api_client.py`, `pyproject.toml`, `jarkdown.py`, and creates new file `field_cache.py` + test files. **File sets are disjoint — verified.**

Design decisions (from 02-CONTEXT.md):
- Cache dir: `~/.config/jarkdown/` via `platformdirs.user_config_dir("jarkdown")`
- One cache file per Jira domain
- 24-hour TTL, re-fetch when stale
- Graceful degradation: stale cache + API failure → use stale data with warning
- CLI flag `--refresh-fields` to force refresh
- CLI flags `--include-fields` and `--exclude-fields` for per-run field filtering

## Tasks

### Task 1: Add new dependencies to pyproject.toml

Add both new dependencies to the `dependencies` list in `pyproject.toml` (after `PyYAML`):
- `"platformdirs>=4.0.0",`
- `'tomli>=2.0.0;python_version<"3.11"',`

Run `pip install -e ".[dev]"` to verify all dependencies resolve.

Commit: `chore(phase-02): add platformdirs and tomli dependencies`

### Task 2: Add fetch_fields() to JiraApiClient

Add a new method to `JiraApiClient` after the existing `download_attachment_stream()` method:

```python
def fetch_fields(self):
    """Fetch all field definitions from Jira.

    Returns:
        list: List of field definition dicts with id, name, schema keys.

    Raises:
        JiraApiError: If the API call fails.
    """
    url = f"{self.api_base}/field"
    self.logger.info("Fetching field metadata...")

    try:
        response = self.session.get(url)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.HTTPError as e:
        if response.status_code == 401:
            raise AuthenticationError(
                "Authentication failed while fetching field metadata.",
                status_code=401,
                response=response,
            )
        raise JiraApiError(
            f"Error fetching field metadata: {e}",
            status_code=response.status_code,
            response=response,
        )
    except requests.exceptions.RequestException as e:
        raise JiraApiError(f"Error fetching field metadata: {e}")
```

Commit: `feat(phase-02): add fetch_fields API method`

### Task 3: Implement FieldMetadataCache class

Create `src/jarkdown/field_cache.py`:

```python
"""Cache for Jira field metadata with XDG-compliant storage."""

import json
import logging
import time
from pathlib import Path

from platformdirs import user_config_dir


class FieldMetadataCache:
    """Manages cached Jira field metadata with 24-hour TTL."""

    CACHE_TTL_SECONDS = 86400  # 24 hours

    def __init__(self, domain):
        """Initialize field metadata cache.

        Args:
            domain: Jira domain for cache isolation (e.g., 'company.atlassian.net')
        """
        self.domain = domain
        self.logger = logging.getLogger(__name__)
        self._cache_dir = Path(user_config_dir("jarkdown"))
        self._cache_file = self._cache_dir / f"fields-{domain}.json"
        self._field_map = None  # Lazy-loaded id→name dict

    def _ensure_cache_dir(self):
        """Create cache directory if it doesn't exist."""
        self._cache_dir.mkdir(parents=True, exist_ok=True)

    def is_stale(self):
        """Check if cached data is older than TTL.

        Returns:
            bool: True if cache doesn't exist or is older than 24 hours.
        """
        if not self._cache_file.exists():
            return True
        try:
            data = json.loads(self._cache_file.read_text())
            cached_at = data.get("cached_at", 0)
            return (time.time() - cached_at) > self.CACHE_TTL_SECONDS
        except (json.JSONDecodeError, OSError):
            return True

    def save(self, fields):
        """Save field metadata to cache with timestamp.

        Args:
            fields: List of field definition dicts from Jira API.
        """
        self._ensure_cache_dir()
        cache_data = {
            "cached_at": time.time(),
            "domain": self.domain,
            "fields": fields,
        }
        self._cache_file.write_text(json.dumps(cache_data, indent=2))
        self._field_map = None  # Reset lazy cache

    def load(self):
        """Load field metadata from cache.

        Returns:
            list: List of field definition dicts, or empty list if no cache.
        """
        if not self._cache_file.exists():
            return []
        try:
            data = json.loads(self._cache_file.read_text())
            return data.get("fields", [])
        except (json.JSONDecodeError, OSError):
            return []

    def get_field_name(self, field_id):
        """Resolve a field ID to its display name.

        Args:
            field_id: Jira field ID (e.g., 'customfield_10001')

        Returns:
            str: Display name or the raw field_id if not found.
        """
        if self._field_map is None:
            fields = self.load()
            self._field_map = {f["id"]: f.get("name", f["id"]) for f in fields if "id" in f}
        return self._field_map.get(field_id, field_id)

    def get_field_schema(self, field_id):
        """Get the schema for a field by ID.

        Args:
            field_id: Jira field ID

        Returns:
            dict: Schema dict with 'type', 'custom' keys, or empty dict.
        """
        if self._field_map is None:
            self.load()
        fields = self.load()
        for f in fields:
            if f.get("id") == field_id:
                return f.get("schema", {})
        return {}

    def refresh(self, api_client, force=False):
        """Refresh cache from API if stale or forced.

        Args:
            api_client: JiraApiClient instance for API calls.
            force: If True, refresh regardless of TTL.

        Returns:
            list: Field metadata list (from fresh fetch or cache).
        """
        if not force and not self.is_stale():
            return self.load()

        try:
            fields = api_client.fetch_fields()
            self.save(fields)
            self.logger.info(f"Field metadata cached ({len(fields)} fields)")
            return fields
        except Exception as e:
            self.logger.warning(f"Failed to refresh field metadata: {e}")
            cached = self.load()
            if cached:
                self.logger.warning("Using stale cached field metadata")
                return cached
            self.logger.warning("No cached field metadata available")
            return []
```

Commit: `feat(phase-02): implement field metadata cache`

### Task 4: Add all new CLI arguments

Add three new arguments to the CLI argument parser in `jarkdown.py`, after the existing `--verbose` argument and before `--version`:

```python
parser.add_argument(
    "--refresh-fields",
    action="store_true",
    help="Force refresh of cached Jira field metadata",
)
parser.add_argument(
    "--include-fields",
    help="Comma-separated list of custom field names to include",
)
parser.add_argument(
    "--exclude-fields",
    help="Comma-separated list of custom field names to exclude",
)
```

These arguments are not wired to export logic yet — Plan 04 (Wave 2) will connect them. The `args.refresh_fields`, `args.include_fields`, and `args.exclude_fields` attributes will be available for Plan 04 to use.

Commit: `feat(phase-02): add field-related CLI arguments`

### Task 5: Write field cache tests

Create `tests/test_field_cache.py`:

```python
@pytest.fixture
def cache(tmp_path, monkeypatch):
    """Create a FieldMetadataCache with temp cache dir."""
    monkeypatch.setattr("jarkdown.field_cache.user_config_dir", lambda _: str(tmp_path))
    return FieldMetadataCache("example.atlassian.net")

@pytest.fixture
def sample_fields():
    """Sample field metadata from Jira API."""
    return [
        {"id": "summary", "name": "Summary", "schema": {"type": "string"}},
        {"id": "customfield_10001", "name": "Story Points", "schema": {"type": "number", "custom": "com.atlassian.jira.plugin.system.customfieldtypes:float"}},
        {"id": "customfield_10002", "name": "Sprint", "schema": {"type": "array", "custom": "com.pyxis.greenhopper.jira:gh-sprint"}},
        {"id": "customfield_10003", "name": "Team", "schema": {"type": "option", "custom": "com.atlassian.jira.plugin.system.customfieldtypes:select"}},
    ]
```

**TestCacheStorage** class:
- `test_save_and_load`: Save fields, load them back, verify content matches
- `test_cache_creates_directory`: Cache dir created automatically
- `test_load_empty_cache`: No cache file → returns empty list
- `test_load_corrupt_cache`: Write invalid JSON to cache file → returns empty list

**TestCacheStaleness** class:
- `test_fresh_cache_not_stale`: Save, immediately check → not stale
- `test_stale_cache`: Save with old timestamp (mock `time.time()`), check → stale
- `test_no_cache_is_stale`: No file → stale
- `test_24h_boundary`: Cache at exactly 24h → stale

**TestFieldNameResolution** class:
- `test_resolve_known_field`: Save fields, resolve `customfield_10001` → "Story Points"
- `test_resolve_unknown_field`: Resolve unknown ID → returns raw ID
- `test_resolve_standard_field`: Resolve `summary` → "Summary"

**TestCacheRefresh** class:
- `test_refresh_when_stale`: Mock API client, verify fetch_fields called and cache updated
- `test_skip_refresh_when_fresh`: Fresh cache → API not called
- `test_force_refresh`: Force=True → API called even when fresh
- `test_graceful_degradation`: API raises exception, stale cache exists → returns stale data with warning

**TestFetchFields** (tests the API client method):
- `test_fetch_fields_success`: Mock session.get → returns field list
- `test_fetch_fields_auth_error`: 401 → raises AuthenticationError
- `test_fetch_fields_api_error`: 500 → raises JiraApiError

Commit: `test(phase-02): add field cache tests`

## Verification

```bash
pytest tests/test_field_cache.py -v
pytest tests/test_components.py -v
pytest tests/test_cli.py -v  # CLI tests still pass with new arguments
```

## Success Criteria

- Both dependencies (platformdirs, tomli) added and installable
- FieldMetadataCache stores/loads field metadata in XDG-compliant directory
- 24-hour TTL works correctly (stale detection)
- Field name resolution maps customfield_XXXXX → display names
- Graceful degradation when API unreachable but stale cache exists
- JiraApiClient.fetch_fields() calls correct endpoint
- All three CLI arguments added to parser
- All tests pass (new and existing)
